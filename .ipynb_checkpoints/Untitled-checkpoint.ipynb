{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "complicated-pharmacy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import signal\n",
    "import pywt\n",
    "\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import h5py\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "keras = tf.keras\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.callbacks import History\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "compliant-publicity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(accDir, annotFile):\n",
    "    files = os.listdir(accDir)\n",
    "    files_csv = [f for f in files if f[-3:] == 'csv']\n",
    "    empatica_dict = dict()\n",
    "    for f in files_csv:\n",
    "        data = np.genfromtxt(accDir+f, delimiter=',') # creates numpy array for each Empatica acc csv file\n",
    "        key = int(float(f.strip(\"ACC.csv\")))\n",
    "        empatica_dict[key] = data\n",
    "    tmp = pd.read_excel(annotFile, sheet_name=None)\n",
    "    annot_dict = dict(zip(tmp.keys(), [i.dropna() for i in tmp.values()])) # Remove the rows with NaN values (some with ladder 2 missing)\n",
    "    return empatica_dict, annot_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "historic-night",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLabeledDict(empatica_dict, annot_dict, subject_ids):\n",
    "    labeled_dict = {}; taskInd_dict = {}\n",
    "    for id in subject_ids:\n",
    "        start_time = int(empatica_dict[id][0,0])\n",
    "        acc = empatica_dict[id][2:,:]\n",
    "        label = list(map(lambda i: i.replace(\"_end\", \"\").replace(\"_start\", \"\"), annot_dict['P'+ str(id)].taskName.tolist()))\n",
    "        task_time= list(map(lambda i: time.mktime(datetime.datetime.strptime(i[:6] + '20' + i[6:], \"%m/%d/%Y %H:%M:%S\").timetuple()),\n",
    "                            annot_dict['P'+ str(id)].startTime_global.tolist()))\n",
    "        task_ind = [int(x - start_time)*SR for x in task_time]\n",
    "        taskInd_dict[id] = task_ind\n",
    "        label_tmp = np.empty(acc.shape[0], dtype=object)\n",
    "        for i, (j, k) in enumerate(zip(task_ind[0::2], task_ind[1::2])):\n",
    "            tmpInd = 2*i\n",
    "            label_tmp[j:k] = label[tmpInd]\n",
    "        acc_mag = np.sqrt(np.sum(acc**2, axis=1))[:,None]\n",
    "        accel = np.hstack((acc, acc_mag))\n",
    "        labeled_dict[id] = pd.DataFrame(np.hstack((accel, label_tmp.reshape(label_tmp.shape[0],1))), columns=['X', 'Y', 'Z', 'Mag', 'label'])\n",
    "    return labeled_dict, taskInd_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "covered-plaintiff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sepAccDict, sepAnnotDict = readData(accDir='./Data/50_subs/Acc Data/separate/', annotFile='./Data/50_subs/Annotation Data/separate.xlsx')\n",
    "SR=int(sepAccDict[8][1,0])\n",
    "\n",
    "sepSubIDs = list(range(8,45))\n",
    "sepLabeledDict_, sepTaskIndDict = getLabeledDict(sepAccDict, sepAnnotDict, sepSubIDs)\n",
    "\n",
    "# Apply Filter on All Subjects\n",
    "n=4; fc=2; w=fc/(SR/2)\n",
    "b, a = signal.butter(n, w, 'low')\n",
    "sepLabeledDict_filtered = dict(map(lambda key: (key, signal.filtfilt(b, a, x=sepLabeledDict_[key].drop(columns='label'), axis=0)), sepLabeledDict_.keys()))\n",
    "# back to DF and add label\n",
    "sepLabeledDict_filtered_dfs = dict(map(lambda key: (\n",
    "                                                        key, pd.DataFrame(sepLabeledDict_filtered[key],columns=['X', 'Y', 'Z', 'Mag']).assign(label=sepLabeledDict_[key].label)\n",
    "                                                    ), sepLabeledDict_filtered.keys()))\n",
    "# Remove data without label\n",
    "filt_noNA_dict = dict(map(lambda key: (key, sepLabeledDict_filtered_dfs[key].dropna()), sepLabeledDict_filtered_dfs.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "professional-ranch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filt_noNA_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "original-honduras",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = ['sit', 'stand', 'walk', 'hoist', 'lift', 'push', 'type', 'ladder1', 'ladder2', 'electricPanel', 'overhead']\n",
    "scales = range(1,200)\n",
    "waveletname = 'morl'\n",
    "coeffs_dict = {}; labels_dict = {}\n",
    "for sub in filt_noNA_dict.keys():\n",
    "    sig_ = filt_noNA_dict[sub]\n",
    "    sig = sig_[sig_.label.isin(tasks)]\n",
    "    sig.label.replace({'ladder1':'ladder', 'ladder2':'ladder'}, inplace=True)\n",
    "\n",
    "    acc_signals = sig[['X', 'Y', 'Z']].values.astype('float')\n",
    "    coeff, freq = pywt.cwt(acc_signals, scales, waveletname, 1/SR, axis=0)\n",
    "    coeffs_dict[sub] = coeff/np.max(coeff)\n",
    "    labels_dict[sub] = sig.label#.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "solid-second",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2021)\n",
    "percentTrain = 80\n",
    "all_subs = list(filt_noNA_dict.keys())\n",
    "train_subs = random.sample(all_subs, k=int(len(all_subs)*(percentTrain/100)))\n",
    "test_subs = list(set(all_subs) - set(train_subs))\n",
    "\n",
    "# with open('test_subs.pickle', 'wb') as outfile:\n",
    "#     pickle.dump(test_subs, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "acute-contamination",
   "metadata": {},
   "outputs": [],
   "source": [
    "winLen = 320\n",
    "########### Train\n",
    "train_features_list = []\n",
    "train_labels_list = []\n",
    "for sub in train_subs:\n",
    "    tmp = labels_dict[sub].ne(labels_dict[sub].shift())\n",
    "    true_segs = list(np.where(tmp)[0])\n",
    "    true_segs.append(len(labels_dict[sub]) - 1)\n",
    "    for i in range(len(true_segs) - 1):\n",
    "        segment_label = labels_dict[sub].values[true_segs[i]]\n",
    "        segment = coeffs_dict[sub][:, true_segs[i]:true_segs[i+1], :]\n",
    "        for j in range(segment.shape[1]//winLen):\n",
    "            train_features_list.append(segment[:, (j*winLen):((j+1)*winLen), :])\n",
    "            train_labels_list.append(segment_label)\n",
    "x_train = np.array(train_features_list)\n",
    "y_train = np.array(train_labels_list)\n",
    "\n",
    "########### Test\n",
    "test_features_list = []\n",
    "test_labels_list = []\n",
    "for sub in test_subs:\n",
    "    tmp = labels_dict[sub].ne(labels_dict[sub].shift())\n",
    "    true_segs = list(np.where(tmp)[0])\n",
    "    true_segs.append(len(labels_dict[sub]) - 1)\n",
    "    for i in range(len(true_segs) - 1):\n",
    "        segment_label = labels_dict[sub].values[true_segs[i]]\n",
    "        segment = coeffs_dict[sub][:, true_segs[i]:true_segs[i+1], :]\n",
    "        for j in range(segment.shape[1]//winLen):\n",
    "            test_features_list.append(segment[:, (j*winLen):((j+1)*winLen), :])\n",
    "            test_labels_list.append(segment_label)\n",
    "x_test = np.array(test_features_list)\n",
    "y_test = np.array(test_labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statutory-example",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-3.8",
   "language": "python",
   "name": "python-3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
