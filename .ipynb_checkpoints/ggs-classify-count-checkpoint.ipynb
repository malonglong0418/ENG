{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-problem",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import signal\n",
    "import pywt\n",
    "\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import h5py\n",
    "import pickle\n",
    "\n",
    "from ggs import *\n",
    "\n",
    "import cv2 as cv\n",
    "\n",
    "from tensorflow.keras import models\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from platform import python_version\n",
    "print(python_version())\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deluxe-payment",
   "metadata": {},
   "source": [
    "# Read and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brilliant-dimension",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(accDir, annotFile):\n",
    "    files = os.listdir(accDir)\n",
    "    files_csv = [f for f in files if f[-3:] == 'csv']\n",
    "    empatica_dict = dict()\n",
    "    for f in files_csv:\n",
    "        data = np.genfromtxt(accDir+f, delimiter=',') # creates numpy array for each Empatica acc csv file\n",
    "        key = int(float(f.strip(\"ACC.csv\")))\n",
    "        empatica_dict[key] = data\n",
    "    tmp = pd.read_excel(annotFile, sheet_name=None)\n",
    "    annot_dict = dict(zip(tmp.keys(), [i.dropna() for i in tmp.values()])) # Remove the rows with NaN values (some with ladder 2 missing)\n",
    "    return empatica_dict, annot_dict\n",
    "\n",
    "def getLabeledDict(empatica_dict, annot_dict, subject_ids, SR):\n",
    "    labeled_dict = {}; taskInd_dict = {}\n",
    "    for id in subject_ids:\n",
    "        start_time = int(empatica_dict[id][0,0])\n",
    "        acc = empatica_dict[id][2:,:]\n",
    "        label = list(map(lambda i: i.replace(\"_end\", \"\").replace(\"_start\", \"\"), annot_dict['P'+ str(id)].taskName.tolist()))\n",
    "        task_time= list(map(lambda i: time.mktime(datetime.datetime.strptime(i[:6] + '20' + i[6:], \"%m/%d/%Y %H:%M:%S\").timetuple()),\n",
    "                            annot_dict['P'+ str(id)].startTime_global.tolist()))\n",
    "        task_ind = [int(x - start_time)*SR for x in task_time]\n",
    "        taskInd_dict[id] = task_ind\n",
    "        label_tmp = np.empty(acc.shape[0], dtype=object)\n",
    "        for i, (j, k) in enumerate(zip(task_ind[0::2], task_ind[1::2])):\n",
    "            tmpInd = 2*i\n",
    "            label_tmp[j:k] = label[tmpInd]\n",
    "        acc_mag = np.sqrt(np.sum(acc**2, axis=1))[:,None]\n",
    "        accel = np.hstack((acc, acc_mag))\n",
    "        labeled_dict[id] = pd.DataFrame(np.hstack((accel, label_tmp.reshape(label_tmp.shape[0],1))), columns=['X', 'Y', 'Z', 'Mag', 'label'])\n",
    "    return labeled_dict, taskInd_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tamil-transmission",
   "metadata": {},
   "outputs": [],
   "source": [
    "sepAccDict, sepAnnotDict = readData(accDir='./Data/50_subs/Acc Data/separate/', annotFile='./Data/50_subs/Annotation Data/separate.xlsx')\n",
    "SR=int(sepAccDict[8][1,0])\n",
    "\n",
    "sepSubIDs = list(range(8,45))\n",
    "# sepSubIDs.remove(27) # does not have lift\n",
    "sepLabeledDict_, sepTaskIndDict = getLabeledDict(sepAccDict, sepAnnotDict, sepSubIDs, SR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-voice",
   "metadata": {},
   "source": [
    "## Apply Low Pass Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "muslim-hammer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Filter on All Subjects\n",
    "n=4; fc=2; w=fc/(SR/2)\n",
    "b, a = signal.butter(n, w, 'low')\n",
    "sepLabeledDict_filtered = dict(map(lambda key: (key, signal.filtfilt(b, a, x=sepLabeledDict_[key].drop(columns='label'), axis=0)), sepLabeledDict_.keys()))\n",
    "# back to DF and add label\n",
    "sepLabeledDict_filtered_dfs = dict(map(lambda key: (\n",
    "                                                        key, pd.DataFrame(sepLabeledDict_filtered[key],columns=['X', 'Y', 'Z', 'Mag']).assign(label=sepLabeledDict_[key].label)\n",
    "                                                    ), sepLabeledDict_filtered.keys()))\n",
    "# Remove data without label\n",
    "filt_noNA_dict = dict(map(lambda key: (key, sepLabeledDict_filtered_dfs[key].dropna()), sepLabeledDict_filtered_dfs.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minus-leadership",
   "metadata": {},
   "source": [
    "## Load and Correct GGS Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-america",
   "metadata": {},
   "outputs": [],
   "source": [
    "segments = pd.read_pickle('segments_all_labels.pickle')\n",
    "new_seg_dict = {}\n",
    "for sub in segments.keys():\n",
    "    segment = segments[sub]\n",
    "    seg = segment[-1]\n",
    "    new_segment = []\n",
    "    \n",
    "    thresh = 960 # ensure 30 seconds between segments\n",
    "    idx = np.where(np.diff(seg) < thresh)[0]\n",
    "    cluster_idxs_ = np.split(idx, np.where(np.diff(idx) != 1)[0]+1)\n",
    "    clusters_idxs = [np.append(elem, max(elem) + 1) for elem in cluster_idxs_]\n",
    "    cluster_vals = [np.array(seg)[idxs] for idxs in clusters_idxs]\n",
    "\n",
    "    new_segs = list(set(seg) - set(np.concatenate(cluster_vals)))\n",
    "    for elem in cluster_vals:\n",
    "        if any(elem==0):\n",
    "            new_segs.append(0)\n",
    "        elif any(elem==seg[-1]):\n",
    "            new_segs.append(seg[-1])\n",
    "        else:\n",
    "            for early_seg in segment:\n",
    "                if any((np.array(early_seg)>(min(elem) - 30)) & (np.array(early_seg)<(max(elem) + 30))):\n",
    "                    replacement = min(np.array(early_seg)[(np.array(early_seg)>(min(elem) - 30)) & (np.array(early_seg)<(max(elem) + 30))])\n",
    "                    new_segs.append(replacement)\n",
    "                    break\n",
    "    new_seg_dict[sub] = sorted(new_segs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "progressive-sight",
   "metadata": {},
   "source": [
    "# Test the Pipeline on Subjects: 8, 9, 10, 11\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "> 1- Extract windows of `10 seconds` within each segment\n",
    "\n",
    "> 2- Perform continuous wavelet transform (`CWT`) in each window (extract features)\n",
    "\n",
    "> 3- Classify each window using the trained `CNN` model\n",
    "\n",
    "> 4- Classify each segment by `voting`\n",
    "\n",
    "> 5- Modify the segments by `merging neighboring windows` of same class\n",
    "\n",
    "> 6- `Count` the repititions in lifting and walking windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepted-subscriber",
   "metadata": {},
   "source": [
    "# Load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "local-swing",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.load_model('./from osc/2021-07-09/tf_model/')\n",
    "test_subs = pd.read_pickle('./from osc/2021-07-09/test_subs.pickle')\n",
    "history = pd.read_pickle('./from osc/2021-07-09/keras_history.pickle')\n",
    "scaler = pd.read_pickle('./from osc/2021-07-09/scaler.pickle')\n",
    "labels_categorical = ['sit', 'stand', 'walk', 'hoist', 'lift', 'push', 'type', 'ladder', 'electricPanel', 'overhead']\n",
    "labels_encoded = LabelEncoder().fit_transform(labels_categorical).tolist()\n",
    "labels_dict = dict([(numeric_, categorical_) for numeric_, categorical_ in zip(labels_encoded, labels_categorical)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fuzzy-victor",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history['accuracy'])\n",
    "print(history['val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assured-rolling",
   "metadata": {},
   "outputs": [],
   "source": [
    "CB_color_cycle = ['#377eb8', '#ff7f00', '#4daf4a', '#f781bf', '#a65628', '#984ea3', '#999999', '#e41a1c', '#dede00']\n",
    "f, ax = plt.subplots(1, figsize=(10, 5))\n",
    "ax.plot(range(1, 11), history['accuracy'], c=CB_color_cycle[0], label='Train', linewidth=3)\n",
    "ax.plot(range(1, 11), history['val_accuracy'], c=CB_color_cycle[4], label='Validation', linewidth=3)\n",
    "ax.set_ylim([0.75, 1])\n",
    "ax.set_xlabel('Epoch Number', fontsize=20)\n",
    "ax.set_ylabel('Accuracy', fontsize=20)\n",
    "ax.set_xticks(range(1, 11))\n",
    "ax.tick_params(labelsize=15)\n",
    "# ax.plot(history['loss'], c=CB_color_cycle[0], label='Train')\n",
    "# ax.plot(history['val_loss'], c=CB_color_cycle[4], label='Validation')\n",
    "\n",
    "ax.legend(fontsize=15)\n",
    "f.tight_layout(pad=0.1)\n",
    "f.savefig('./outputs/CNN classification/accuracy.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considered-council",
   "metadata": {},
   "source": [
    "## Automated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-composer",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwt_dict = {}\n",
    "for sub in sorted(test_subs):\n",
    "    tasks = ['sit', 'stand', 'walk', 'hoist', 'lift', 'push', 'type', 'ladder1', 'ladder2', 'electricPanel', 'overhead']\n",
    "    sig_ = filt_noNA_dict[sub]\n",
    "    sig = sig_[sig_.label.isin(tasks)]\n",
    "    sig.label.replace({'ladder1':'ladder', 'ladder2':'ladder'}, inplace=True)\n",
    "\n",
    "    winLen = 320\n",
    "    scales = range(1,200)\n",
    "    waveletname = 'morl'\n",
    "    cwt_dict[sub] = {}\n",
    "    for label, label_df in sig.groupby(by='label'):\n",
    "        cwt_dict[sub][label] = np.zeros((label_df.shape[0]//winLen, max(scales), winLen, 3))\n",
    "        j = 0\n",
    "        for window, window_df in label_df.groupby(by=np.arange(label_df.shape[0])//winLen):\n",
    "            if window_df.shape[0] == winLen:\n",
    "                ########### cwt transform\n",
    "                for i in range(3):\n",
    "                    tmp_sig = window_df.values[:,i]\n",
    "                    coeff, freq = pywt.cwt(tmp_sig, scales, waveletname, 1/SR)\n",
    "                    cwt_dict[sub][label][j, :,:,i] = coeff\n",
    "                j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "exempt-drawing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CountReps(axis, new_segs, sig, wavename, wave_coeff_perc, scale_interv, area_thresh_coeff, num_contours, SR, scales):\n",
    "    sig_label = sig[axis].values[new_segs[idx]:new_segs[idx + 1]]\n",
    "    [coefficients, frequencies] = pywt.cwt(sig_label, scales, wavename, sampling_period=1/SR)\n",
    "    thresh = np.percentile(abs(coefficients), wave_coeff_perc)\n",
    "    ret, thresh_img__ = cv.threshold(abs(coefficients), thresh, 255, cv.THRESH_BINARY)\n",
    "    thresh_img_ = thresh_img__.astype('uint8')\n",
    "    thresh_img = thresh_img_[scale_interv,:]\n",
    "    contours, hierarchy = cv.findContours(thresh_img.copy(), cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
    "    area = [cv.contourArea(cnt) for cnt in contours]\n",
    "    area_thresh = np.percentile(area, 90)*area_thresh_coeff\n",
    "    contours_np = np.array(contours)\n",
    "    area_np = np.array(area)\n",
    "    large_contours = contours_np[np.where(area_np>area_thresh)]\n",
    "    return int(round(large_contours.shape[0]/num_contours, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "minus-coral",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_segs_dict = {}\n",
    "new_labels_dict = {}\n",
    "act_count_dict = {}\n",
    "for sub in sorted(test_subs):\n",
    "    segs = new_seg_dict[sub]\n",
    "    # tasks = ['sit', 'stand', 'walk', 'hoist', 'lift', 'ladder1']\n",
    "    tasks = ['sit', 'stand', 'walk', 'hoist', 'lift', 'push', 'type', 'ladder1', 'ladder2', 'electricPanel', 'overhead']\n",
    "    sig_ = filt_noNA_dict[sub]\n",
    "    sig = sig_[sig_.label.isin(tasks)]\n",
    "\n",
    "    winLen = 320\n",
    "    scales = range(1,200)\n",
    "    waveletname = 'morl'\n",
    "    cwt_dict = {}\n",
    "    for i in range(len(segs)-1):\n",
    "        ########### create window of length 320\n",
    "        signal_seg = sig.iloc[segs[i]:segs[i+1],:3]\n",
    "        window_list = []\n",
    "        for g, df in signal_seg.groupby(np.arange(signal_seg.shape[0]) // winLen):\n",
    "            if df.shape[0]==winLen:\n",
    "                window_list.append(df.values)\n",
    "        window_np = np.array(window_list)\n",
    "        ########### cwt transform\n",
    "        cwt_dict[i] = np.zeros(shape=(window_np.shape[0], max(scales), winLen, 3), dtype=np.float)\n",
    "        for j in range(window_np.shape[0]):\n",
    "            for k in range(window_np.shape[2]):\n",
    "                tmp_sig = window_np[j,:,k]\n",
    "                coeff, freq = pywt.cwt(tmp_sig, scales, waveletname, 1/SR)\n",
    "                cwt_dict[i][j, :, :, k] = coeff\n",
    "\n",
    "    ############## predict labels within each segment using the trained model and voting\n",
    "    scaler = max([np.max(elem) for elem in cwt_dict.values()])\n",
    "    pred_label = dict()\n",
    "    for key in cwt_dict.keys():\n",
    "        OH_pred = model.predict(cwt_dict[key]/scaler)\n",
    "        numeric_pred = np.argmax(OH_pred, axis=1)\n",
    "        most_frequent = np.bincount(numeric_pred).argmax()\n",
    "        pred_label[key] = labels_dict[most_frequent]\n",
    "\n",
    "    ############## remove segmenets for same neighbouring labels\n",
    "    labels = list(pred_label.values())\n",
    "    new_segs = [segs[0], segs[-1]]\n",
    "    new_labels = [labels[0]]\n",
    "    for i in range(len(labels[:-1])):\n",
    "        if labels[i] != labels[i+1]:\n",
    "            new_segs.append(segs[i+1])\n",
    "            new_labels.append(labels[i+1])\n",
    "    new_segs = sorted(new_segs)\n",
    "    new_segs_dict[sub] = new_segs\n",
    "    new_labels_dict[sub] = np.array(new_labels)\n",
    "    \n",
    "    ############## Count number of Repetitions in `Walkin` and `Lifting`\n",
    "    count_dict = {}\n",
    "    labels = ['walk', 'lift', 'hoist']\n",
    "    for label in labels:\n",
    "        count_dict[label] = []\n",
    "        Idxs = []\n",
    "        Idxs.extend(list(np.where(np.array(new_labels)==label)[0]))\n",
    "        for idx in Idxs:\n",
    "            if label=='walk':\n",
    "                seg_reps = CountReps(axis='Y', new_segs=new_segs, sig=sig, wavename='morl', wave_coeff_perc=70,\n",
    "                                     scale_interv=slice(20, 35), area_thresh_coeff=0.3, num_contours=2, SR=SR, scales=scales)\n",
    "            elif label=='lift':\n",
    "                seg_reps = CountReps(axis='Y', new_segs=new_segs, sig=sig, wavename='morl', wave_coeff_perc=80,\n",
    "                                     scale_interv=slice(90, None), area_thresh_coeff=0.3, num_contours=4, SR=SR, scales=scales)\n",
    "            elif label=='hoist':\n",
    "                seg_reps = CountReps(axis='Y', new_segs=new_segs, sig=sig, wavename='gaus1', wave_coeff_perc=40,\n",
    "                                     scale_interv=slice(None, 30), area_thresh_coeff=0.1, num_contours=2, SR=SR, scales=scales)\n",
    "            count_dict[label].append(seg_reps)\n",
    "            act_count_dict[sub] = count_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyzed-myanmar",
   "metadata": {},
   "source": [
    "## Plot with Modified Segments, Predicted Labels, and Counts for Test Subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "younger-attempt",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3652f110e534655a715dc05bd625ceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(Output(), Output(), Output(), Output(), Output(), Output(), Output(), Output()), _titles={'0': '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CB_color_cycle = ['#377eb8', '#ff7f00', '#4daf4a', '#f781bf', '#a65628', '#984ea3', '#999999', '#e41a1c', '#dede00']\n",
    "subs = sorted(test_subs)\n",
    "sub_tab=[widgets.Output() for i in range(len(subs))]\n",
    "tab = widgets.Tab(sub_tab)\n",
    "for i, sub in enumerate(subs):\n",
    "    tab.set_title(i,str(sub))\n",
    "    sig_ = filt_noNA_dict[sub]\n",
    "    sig = sig_[sig_.label.isin(tasks)]\n",
    "    parint_labels = [label.replace('electricPanel', 'EP').replace('overhead', 'OH') for label in new_labels_dict[sub]]\n",
    "    with sub_tab[i]:\n",
    "        f, ax = plt.subplots(1, figsize=(20,5))\n",
    "        ax.plot(sig.values[:,0], c=CB_color_cycle[0], label='X')\n",
    "        ax.plot(sig.values[:,1], c=CB_color_cycle[4], label='Y')\n",
    "        ax.plot(sig.values[:,2], c=CB_color_cycle[8], label='Z')\n",
    "        for v in new_segs_dict[sub]:\n",
    "            ax.axvline(v, c='k', linestyle = '--')\n",
    "        ax.set_ylim([-150, 110])\n",
    "        x_locs = (np.array(new_segs_dict[sub][:-1]) + np.array(new_segs_dict[sub][1:])) // 2\n",
    "        y_locs = ([130, 150] * 10)[:new_labels_dict[sub].shape[0]]\n",
    "        walk_idx = 0\n",
    "        lift_idx = 0\n",
    "        hoist_idx = 0\n",
    "        for x_loc, y_loc, label in zip(x_locs, y_locs, parint_labels):\n",
    "            ax.text(x_loc-1500, y_loc, label, fontsize=20)\n",
    "            if label=='walk':\n",
    "                ax.text(x_loc-1500, y_loc-15, '(' + str(act_count_dict[sub]['walk'][walk_idx]) + ')', fontsize=15)\n",
    "                walk_idx += 1\n",
    "            if label=='lift':\n",
    "                ax.text(x_loc-1500, y_loc-15, '(' + str(act_count_dict[sub]['lift'][lift_idx]) + ')', fontsize=15)\n",
    "                lift_idx += 1\n",
    "            if label=='hoist':\n",
    "                ax.text(x_loc-1500, y_loc-15, '(' + str(act_count_dict[sub]['hoist'][hoist_idx]) + ')', fontsize=15)\n",
    "                hoist_idx += 1\n",
    "        ax.legend()\n",
    "        plt.show()\n",
    "display(tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spanish-falls",
   "metadata": {},
   "source": [
    "# Export Figure for Wole Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "major-headquarters",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub = 8\n",
    "for sub in test_subs:\n",
    "    CB_color_cycle = ['#377eb8', '#ff7f00', '#4daf4a', '#f781bf', '#a65628', '#984ea3', '#999999', '#e41a1c', '#dede00']\n",
    "    tasks = ['sit', 'stand', 'walk', 'hoist', 'lift', 'push', 'type', 'ladder1', 'ladder2', 'electricPanel', 'overhead']\n",
    "    sig_ = filt_noNA_dict[sub]\n",
    "    sig = sig_[sig_.label.isin(tasks)]\n",
    "    parint_labels = [label.replace('electricPanel', 'EP').replace('overhead', 'OH') for label in new_labels_dict[sub]]\n",
    "\n",
    "    f, ax = plt.subplots(1, figsize=(20,3.5))\n",
    "    ax.plot(sig.values[:,0]*9.81/64, c=CB_color_cycle[0], label='$a_X$')\n",
    "    ax.plot(sig.values[:,1]*9.81/64, c=CB_color_cycle[4], label='$a_Y$')\n",
    "    ax.plot(sig.values[:,2]*9.81/64, c=CB_color_cycle[8], label='$a_Z$')\n",
    "    for v in new_segs_dict[sub]:\n",
    "        ax.axvline(v, c='k', linestyle = '--')\n",
    "    ax.set_ylim([-23, 17])\n",
    "    ax.tick_params(axis='both', labelsize=15)\n",
    "    ax.legend(loc='lower left', fontsize=22)\n",
    "    # ax.set_title('Test Subject 1', fontsize=17)\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_ylabel('Acceleration ($m/s^2$)', fontsize=22)\n",
    "\n",
    "    x_locs = (np.array(new_segs_dict[sub][:-1]) + np.array(new_segs_dict[sub][1:])) // 2\n",
    "    y_locs = ([20.5, 22] * 10)[:new_labels_dict[sub].shape[0]]\n",
    "    walk_idx = 0\n",
    "    lift_idx = 0\n",
    "    hoist_idx = 0\n",
    "    for x_loc, y_loc, label in zip(x_locs, y_locs, parint_labels):\n",
    "        ax.text(x_loc-1500, y_loc, label, fontsize=20)\n",
    "    #     if label=='walk':\n",
    "    #         ax.text(x_loc-1500, y_loc-2.3, '(' + str(act_count_dict[sub]['walk'][walk_idx]) + ')', fontsize=15)\n",
    "    #         walk_idx += 1\n",
    "        if label=='lift':\n",
    "            ax.text(x_loc-1500, y_loc-2.5, '(' + str(act_count_dict[sub]['lift'][lift_idx]) + ')', fontsize=15)\n",
    "            lift_idx += 1\n",
    "        if label=='hoist':\n",
    "            ax.text(x_loc-1000, y_loc-2.5, '(' + str(act_count_dict[sub]['hoist'][hoist_idx]) + ')', fontsize=15)\n",
    "            hoist_idx += 1\n",
    "\n",
    "    f.tight_layout(pad=0)#.1)\n",
    "    f.savefig('./outputs/framework/framework' + str(sub) + '.png', bbox_inches=\"tight\")\n",
    "#     plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-candle",
   "metadata": {},
   "source": [
    "# Export Figure for Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-coach",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seg_sub_list = 8\n",
    "for sub in test_subs:\n",
    "    CB_color_cycle = ['#377eb8', '#ff7f00', '#4daf4a', '#f781bf', '#a65628', '#984ea3', '#999999', '#e41a1c', '#dede00']\n",
    "    tasks = ['sit', 'stand', 'walk', 'hoist', 'lift', 'push', 'type', 'ladder1', 'ladder2', 'electricPanel', 'overhead']\n",
    "    sig_ = filt_noNA_dict[sub]\n",
    "    sig = sig_[sig_.label.isin(tasks)]\n",
    "    sig.label.replace({'ladder1':'ladder', 'ladder2':'ladder'}, inplace=True)\n",
    "\n",
    "    tmp = sig.ne(sig.shift()).label.values\n",
    "    true_segs = list(np.where(tmp)[0])\n",
    "    true_segs.append(sig.shape[0])\n",
    "\n",
    "    f, ax = plt.subplots(1, figsize=(20,3.5))\n",
    "    ax.plot(sig.values[:,0]*9.81/64, c=CB_color_cycle[0])#, label='$a_X$')\n",
    "    ax.plot(sig.values[:,1]*9.81/64, c=CB_color_cycle[4])#, label='$a_Y$')\n",
    "    ax.plot(sig.values[:,2]*9.81/64, c=CB_color_cycle[8])#, label='$a_Z$')\n",
    "    for v in new_segs_dict[sub]:\n",
    "        ax.axvline(v, c='b', linewidth=2.5)#, marker='|', label='predicted')\n",
    "    ax.axvline(c='b', linewidth=2.5, label='predicted')\n",
    "    for v in true_segs:\n",
    "        ax.axvline(v, c='red', linewidth=2.5, linestyle = '--')#, label='true')\n",
    "    ax.axvline(c='red', linewidth=2.5, linestyle = '--', label='true')\n",
    "    ax.set_ylim([-23, 17])\n",
    "    ax.tick_params(axis='both', labelsize=15)\n",
    "    ax.legend(loc='lower left', fontsize=22)\n",
    "    # ax.set_title('Test Subject 1', fontsize=17)\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_ylabel('Acceleration ($m/s^2$)', fontsize=22)\n",
    "\n",
    "\n",
    "    f.tight_layout(pad=0)#.1)\n",
    "    f.savefig('./outputs/segmentation/segments' + str(sub) + '.png', bbox_inches=\"tight\")\n",
    "#     plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welcome-privilege",
   "metadata": {},
   "source": [
    "# Covering Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-fountain",
   "metadata": {},
   "outputs": [],
   "source": [
    "covering_metric = {}\n",
    "for sub in sorted(test_subs):\n",
    "    sig_ = filt_noNA_dict[sub]\n",
    "    sig = sig_[sig_.label.isin(tasks)]\n",
    "    sig.loc[sig.label.isin(['ladder1', 'ladder2']), 'label'] = 'ladder'\n",
    "\n",
    "    tmp = sig.ne(sig.shift()).label.values\n",
    "    true_segs = list(np.where(tmp)[0])\n",
    "    true_segs.append(sig.shape[0])\n",
    "\n",
    "    truth = [list(range(st, ed)) for st, ed in zip(true_segs[:-1], true_segs[1:])]\n",
    "    pred = [list(range(st, ed)) for st, ed in zip(new_segs_dict[sub][:-1], new_segs_dict[sub][1:])]\n",
    "    \n",
    "    summation = 0\n",
    "    for t in truth:\n",
    "        jaccard = []\n",
    "        for p in pred:\n",
    "            intersect = list(set(t) & set(p))\n",
    "            union = list(set(t+p))\n",
    "            jaccard.append(len(intersect)/len(union))\n",
    "        summation += len(t) * max(jaccard)\n",
    "    covering_metric[sub] = summation/sig.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-cigarette",
   "metadata": {},
   "outputs": [],
   "source": [
    "covering_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fiscal-interim",
   "metadata": {},
   "outputs": [],
   "source": [
    "# true_walk_count = [164, 157, 168, 174, 150, 189, 160, 185]\n",
    "# true_lift_count = [20, 20, 20, 20, 20, 20, 20, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "critical-tablet",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Test Subjects', 'Subject Classification Accuracy', 'Segment Classification Accuracy after Voting', 'Segmentation Covering Metric', 'Walking Counting Error', 'Lifting Counting Error']\n",
    "df = pd.DataFrame(index=sorted(test_subs), columns=cols)\n",
    "df['Test Subjects'] = sorted(test_subs)\n",
    "df['Subject Classification Accuracy'] = [0.96, 0.95, 0.73, 0.99, 0.99, 0.97, 0.88, 0.96]\n",
    "df['Segment Classification Accuracy after Voting'] = [1, 1, 0.9, 1, 1, 1, 0.9, 1]\n",
    "df['Segmentation Covering Metric'] = covering_metric.values()\n",
    "df['Walking Counting Error'] = [0.02, 0.06, 0.00, 0.06, 0.01, 0.07, 0.23, 0.03]\n",
    "df['Lifting Counting Error'] = [0.00, 0.05, 0.00, 0.00, 0.00, 0.00, 0.00, 0.05]\n",
    "df.round(decimals=2)\n",
    "# df.round(decimals=2).to_csv('./outputs/evaluation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-monday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub = 8\n",
    "# segs = new_seg_dict[sub]\n",
    "# # tasks = ['sit', 'stand', 'walk', 'hoist', 'lift', 'ladder1']\n",
    "# tasks = ['sit', 'stand', 'walk', 'hoist', 'lift', 'push', 'type', 'ladder1', 'ladder2', 'electricPanel', 'overhead']\n",
    "# sig_ = filt_noNA_dict[sub]\n",
    "# sig = sig_[sig_.label.isin(tasks)]\n",
    "\n",
    "# winLen = 320\n",
    "# scales = range(1,winLen)\n",
    "# waveletname = 'morl'\n",
    "# cwt_dict = {}\n",
    "# for i in range(len(segs)-1):\n",
    "#     ########### create window of length 320\n",
    "#     signal_seg = sig.iloc[segs[i]:segs[i+1],:3]\n",
    "#     window_list = []\n",
    "#     for g, df in signal_seg.groupby(np.arange(signal_seg.shape[0]) // winLen):\n",
    "#         if df.shape[0]==winLen:\n",
    "#             window_list.append(df.values)\n",
    "#     window_np = np.array(window_list)\n",
    "#     ########### cwt transform\n",
    "#     cwt_dict[i] = np.zeros(shape=(window_np.shape[0], winLen-1, winLen-1, 3), dtype=np.float)\n",
    "#     for j in range(window_np.shape[0]):\n",
    "#         for k in range(window_np.shape[2]):\n",
    "#             tmp_sig = window_np[j,:,k]\n",
    "#             coeff, freq = pywt.cwt(tmp_sig, scales, waveletname, 1/SR)\n",
    "#             cwt_dict[i][j, :, :, k] = coeff[:, :-1]\n",
    "            \n",
    "# ############## predict labels within each segment using the trained model and voting\n",
    "# scaler = max([np.max(elem) for elem in cwt_dict.values()])\n",
    "# pred_label = dict()\n",
    "# for key in cwt_dict.keys():\n",
    "#     OH_pred = model.predict(cwt_dict[key]/scaler)\n",
    "#     numeric_pred = np.argmax(OH_pred, axis=1)\n",
    "#     most_frequent = np.bincount(numeric_pred).argmax()\n",
    "#     pred_label[key] = labels_dict[most_frequent]\n",
    "\n",
    "# ############## remove segmenets for same neighbouring labels\n",
    "# labels = list(pred_label.values())\n",
    "# new_segs = [segs[0], segs[-1]]\n",
    "# new_labels = [labels[0]]\n",
    "# for i in range(len(labels[:-1])):\n",
    "#     if labels[i] != labels[i+1]:\n",
    "#         new_segs.append(segs[i+1])\n",
    "#         new_labels.append(labels[i+1])\n",
    "# new_segs = sorted(new_segs)\n",
    "# new_labels = np.array(new_labels)\n",
    "\n",
    "# ############## Count number of Repetitions in `Walkin` and `Lifting`\n",
    "# count_dict = {}\n",
    "# labels = ['walk', 'lift']\n",
    "# for label in labels:\n",
    "#     count_dict[label] = []\n",
    "#     Idxs = []\n",
    "#     Idxs.append(np.where(new_labels==label)[0][0])\n",
    "#     for idx in Idxs:\n",
    "#         sig_label = sig.Y.values[new_segs[idx]:new_segs[idx + 1]]\n",
    "#         scales = np.arange(1, 200)\n",
    "#         [coefficients, frequencies] = pywt.cwt(sig_label, scales, 'morl', sampling_period=1/SR)\n",
    "#         power = (abs(coefficients)) ** 2\n",
    "#         power_20perc = np.percentile(power, 80)\n",
    "#         ret, thresh_img__ = cv.threshold(power, power_20perc, 255, cv.THRESH_BINARY)\n",
    "#         thresh_img_ = thresh_img__.astype('uint8')\n",
    "#         if label=='walk':\n",
    "#             thresh_img = thresh_img_[20:35,:]\n",
    "#         elif label=='lift':\n",
    "#             thresh_img = thresh_img_[90:,:]\n",
    "\n",
    "#         contours, hierarchy = cv.findContours(thresh_img.copy(), cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
    "#         area = [cv.contourArea(cnt) for cnt in contours]\n",
    "#         area_thresh = np.percentile(area, 90)*0.3\n",
    "#         contours_np = np.array(contours)\n",
    "#         area_np = np.array(area)\n",
    "#         large_contours = contours_np[np.where(area_np>area_thresh)]\n",
    "#         if label=='walk':\n",
    "#             count_dict[label].append(int(round(large_contours.shape[0]/2, 0)))\n",
    "#         elif label=='lift':\n",
    "#             count_dict[label].append(int(round(large_contours.shape[0]/4, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-ballet",
   "metadata": {},
   "source": [
    "# Counting by FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-florence",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fftpack import fft\n",
    "from scipy.fftpack import ifft\n",
    "from detecta import detect_peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unable-gamma",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = fft(task_sig)\n",
    "type(aa), aa.shape, aa.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "further-separate",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = 8\n",
    "new_segs_dict[sub]\n",
    "sig_ = filt_noNA_dict[sub]\n",
    "sig = sig_[sig_.label.isin(tasks)][['X', 'Y', 'Z']].values\n",
    "\n",
    "walk_sig = sig[new_segs_dict[sub][2]:new_segs_dict[sub][3],1]\n",
    "hoist_sig = sig[new_segs_dict[sub][3]:new_segs_dict[sub][4],1]\n",
    "lift_sig = sig[new_segs_dict[sub][4]:new_segs_dict[sub][5],1]\n",
    "ladder_sig = sig[new_segs_dict[sub][7]:new_segs_dict[sub][8],1]\n",
    "\n",
    "####################### Task\n",
    "task_sig = ladder_sig\n",
    "\n",
    "f_values = np.linspace(0, SR/2, task_sig.shape[0]//2)\n",
    "fft_values = 2/task_sig.shape[0] * np.abs(fft(task_sig)[0:task_sig.shape[0]//2])\n",
    "\n",
    "max_peak_height = 0.2 * np.nanmax(fft_values)\n",
    "peaks = detect_peaks(fft_values, mph = max_peak_height)\n",
    "print('peack frequency = ', f_values[peaks])\n",
    "\n",
    "num_rep = (task_sig.shape[0]/SR)* max(f_values[peaks])\n",
    "print('Task repeats {} times.'.format(np.floor(num_rep)))\n",
    "\n",
    "f, ax = plt.subplots(2, 1, figsize=(20,10))\n",
    "ax[0].plot(task_sig)\n",
    "ax[1].plot(f_values, fft_values)#, '.-')\n",
    "# ax[1].set_ylim([-5, 600])\n",
    "ax[1].set_xlim([-0.1, 3])\n",
    "ax[1].scatter(f_values[peaks], fft_values[peaks], marker='*', c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharp-structure",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "familiar-scientist",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "about-shareware",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-movie",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electrical-alliance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abandoned-johns",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spatial-election",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optical-narrow",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recorded-feelings",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-3.8",
   "language": "python",
   "name": "python-3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
